{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import itertools\n",
    "import cv2\n",
    "import tensorflow as tf \n",
    "\n",
    "import keras.backend as K\n",
    "from tensorflow.keras.applications import MobileNetV2\n",
    "import matplotlib.pyplot as plt\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Загрузка моделей"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`input_shape` is undefined or non-square, or `rows` is not in [96, 128, 160, 192, 224]. Weights for input shape (224, 224) will be loaded as the default.\n"
     ]
    }
   ],
   "source": [
    "base_model=MobileNetV2(input_shape=[256, 256, 3], include_top=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Слои"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the activations of these layers\n",
    "layer_names = [\n",
    "    'block_1_expand_relu',   # 64x64\n",
    "    'block_3_expand_relu',   # 32x32\n",
    "    'block_6_expand_relu',   # 16x16\n",
    "    'block_13_expand_relu',  # 8x8\n",
    "    'block_16_project',      # 4x4\n",
    "]\n",
    "base_model_outputs = [base_model.get_layer(name).output for name in layer_names]\n",
    "# Create the feature extraction model\n",
    "down_stack = tf.keras.Model(inputs=base_model.input, outputs=base_model_outputs)\n",
    "down_stack.trainable = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Модель функции"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def upsample(filters, size, norm_type='batchnorm', apply_dropout=False):\n",
    "    initializer = tf.random_normal_initializer(0., 0.02)\n",
    "    result = tf.keras.Sequential()\n",
    "    result.add(\n",
    "      tf.keras.layers.Conv2DTranspose(filters, size, strides=2,\n",
    "                                      padding='same',\n",
    "                                      kernel_initializer=initializer,\n",
    "                                      use_bias=False))\n",
    "\n",
    "    if norm_type.lower() == 'batchnorm':\n",
    "        result.add(tf.keras.layers.BatchNormalization())\n",
    "    elif norm_type.lower() == 'instancenorm':\n",
    "        result.add(InstanceNormalization())\n",
    "\n",
    "    if apply_dropout:\n",
    "        result.add(tf.keras.layers.Dropout(0.5))\n",
    "\n",
    "        result.add(tf.keras.layers.ReLU())\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "up_stack = [\n",
    "    upsample(512, 3),  # 4x4 -> 8x8\n",
    "    upsample(256, 3),  # 8x8 -> 16x16\n",
    "    upsample(128, 3),  # 16x16 -> 32x32\n",
    "    upsample(64, 3),   # 32x32 -> 64x64\n",
    "]\n",
    "\n",
    "def unet_model(output_channels):\n",
    "    inputs = tf.keras.layers.Input(shape=[256, 256, 3])\n",
    "\n",
    "    # Downsampling through the model\n",
    "    skips = down_stack(inputs)\n",
    "    x = skips[-1]\n",
    "    skips = reversed(skips[:-1])\n",
    "\n",
    "  # Upsampling and establishing the skip connections\n",
    "    for up, skip in zip(up_stack, skips):\n",
    "        x = up(x)\n",
    "        concat = tf.keras.layers.Concatenate()\n",
    "        x = concat([x, skip])\n",
    "\n",
    "  # This is the last layer of the model\n",
    "    last = tf.keras.layers.Conv2DTranspose(\n",
    "      output_channels, 3, strides=2, activation='sigmoid',\n",
    "      padding='same')  #64x64 -> 128x128\n",
    "\n",
    "    x = last(x)\n",
    "\n",
    "    return tf.keras.Model(inputs=inputs, outputs=x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = unet_model(1)\n",
    "model.load_weights('Unet-disk-model.h5')\n",
    "\n",
    "def get_rectangle(image):\n",
    "    global model  # чтобы каждый раз во время тестирования не подгружать модель - начинает выкидывать warning-и\n",
    "    car_image = tf.image.resize(image, [256,256])\n",
    "    car_image = tf.cast(car_image, tf.float32) / 255.0  \n",
    "    pred_mask = model.predict(car_image[tf.newaxis, ...],verbose=1,steps=1)\n",
    "    pred_mask=pred_mask.reshape(256,256,1)   \n",
    "    new_img=tf.keras.preprocessing.image.array_to_img(pred_mask)\n",
    "    binary = np.array(new_img)\n",
    "    contours, hierarchy =  cv2.findContours(binary, cv2.RETR_LIST, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    cnts = sorted(contours, key = cv2.contourArea, reverse = True)[:2]\n",
    "    for c in cnts:\n",
    "        perimeter = cv2.arcLength(c, True)\n",
    "        area = cv2.contourArea(c)\n",
    "        if area<5*10**4:\n",
    "            accuracy = 0.001 * cv2.arcLength(c, True)\n",
    "            approx = cv2.approxPolyDP(c, accuracy, True)\n",
    "    minx,maxx,miny,maxy=260,0,260,0\n",
    "    for c_ in c:\n",
    "        minx=min(minx,c_[:,0])\n",
    "        miny=min(miny,c_[:,1])\n",
    "        maxx=max(maxx,c_[:,0])\n",
    "        maxy=max(maxy,c_[:,1])\n",
    "    return [minx,maxx,miny,maxy]   \n",
    "    \n",
    "\n",
    "def func_to_test(image, coords, dim):\n",
    "    \n",
    "    image=cv2.imread('C:/Users/leh-1/Documents/TrainYourOwnYOLO/Data/Source_Images/Test_Images/' + image)\n",
    "    \n",
    "    if dim == 1:\n",
    "        first_tire = coords[0]\n",
    "        rimage1 = image[first_tire[2]:first_tire[3],first_tire[0]:first_tire[1]]\n",
    "        return [get_rectangle(rimage1)]\n",
    "    \n",
    "    if dim == 2:\n",
    "        first_tire = coords[0]\n",
    "        second_tire = coords[1]\n",
    "        rimage1 = image[first_tire[2]:first_tire[3],first_tire[0]:first_tire[1]]\n",
    "        rimage2 = image[second_tire[2]:second_tire[3],second_tire[0]:second_tire[1]]\n",
    "        return [get_rectangle(rimage1), get_rectangle(rimage2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytest\n",
    "import ipytest\n",
    "\n",
    "ipytest.config.rewrite_asserts = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# выгрузка csv, полученной от YOLO\n",
    "csv = pd.read_csv('C:/Users/leh-1/Documents/TrainYourOwnYOLO/3_Inference/out.csv')\n",
    "df = csv[['image', 'coords']]\n",
    "df.loc[:]['image'] = df['image'].str.split('\\\\').str.get(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "@pytest.mark.parametrize(\"img\", df['image'].to_list())\n",
    "def test_func_to_test(img):\n",
    "    \n",
    "    global df\n",
    "    \n",
    "    dimension = df[df['image'] == img]['coords'].to_list()[0].count('[') - 1  # проверка количества найденных YOLO колёс\n",
    "    \n",
    "    if dimension == 0:\n",
    "        assert True\n",
    "        return None\n",
    "    \n",
    "    if dimension == 1:\n",
    "        list_raw = df[df['image'] == img]['coords'].to_list()[0].replace('[', '').replace(']','').replace(' ', '').split(',')\n",
    "        l_left = []\n",
    "        for num in list_raw[0:4]:\n",
    "            l_left.append(int(num))\n",
    "        coordinates = [l_left]\n",
    "    \n",
    "    if dimension == 2:\n",
    "        l_left = []\n",
    "        l_right = []\n",
    "        list_raw = df[df['image'] == img]['coords'].to_list()[0].replace('[', '').replace(']','').replace(' ', '').split(',')\n",
    "        for num in list_raw[0:4]:\n",
    "            l_left.append(int(num))\n",
    "        for num in list_raw[4:8]:\n",
    "            l_right.append(int(num))\n",
    "        \n",
    "        coordinates = [l_left, l_right]\n",
    "\n",
    "    List_out = func_to_test(img, coordinates, dimension)\n",
    "    assert len(List_out) == dimension\n",
    "    \n",
    "    crds = []\n",
    "    for c in range(dimension):\n",
    "        crd = []\n",
    "        for i in range(4):\n",
    "            crd.append(int(List_out[c][i]))\n",
    "        crds.append(crd)\n",
    "    final_dict[img] = crds\n",
    "        \n",
    "    #list_first_wheel = []\n",
    "    #list_second_wheel = []\n",
    "    #for i in range(4):\n",
    "    #    list_first_wheel.append(l1[i][0])\n",
    "    #    list_second_wheel.append(l2[i][0])\n",
    "    #final_dict[img] = List_out\n",
    "# тест проверяет, что итогом работы функции стало то количество спиское, что и было на выходе YOLO\n",
    "# и каждый из них содержит 4 значения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..................................................                                           [100%]\n"
     ]
    }
   ],
   "source": [
    "# точка - удачный тест, 'F' - тест закончился ошибкой\n",
    "final_dict = {}\n",
    "ipytest.run('-qq')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'00021.jpg': [[0, 255, 208, 255], [0, 109, 237, 255]],\n",
       " '00022.jpg': [[18, 67, 79, 153], [223, 255, 23, 57]],\n",
       " '00027.jpg': [[145, 193, 32, 43], [73, 165, 215, 233]],\n",
       " '00033.jpg': [[139, 239, 7, 75], [175, 207, 245, 255]],\n",
       " '00037.jpg': [[121, 221, 14, 43], [3, 51, 33, 65]],\n",
       " '00042.jpg': [[0, 49, 197, 241], [31, 63, 217, 255]],\n",
       " '00052.jpg': [[0, 64, 16, 51], [41, 123, 0, 38]],\n",
       " '00054.jpg': [[9, 31, 65, 117], [15, 31, 77, 127]],\n",
       " '00072.jpg': [[0, 255, 176, 255], [199, 239, 96, 197]],\n",
       " '00077.jpg': [[173, 255, 151, 231], [179, 255, 0, 127]],\n",
       " '00086.jpg': [[111, 255, 0, 175], [0, 19, 165, 211]],\n",
       " '00087a6bd4dc_03.jpg': [[0, 81, 0, 47], [231, 255, 63, 121]],\n",
       " '00087a6bd4dc_04.jpg': [[0, 255, 0, 47], [0, 255, 0, 79]],\n",
       " '00087a6bd4dc_05.jpg': [[0, 41, 17, 49], [97, 129, 33, 40]],\n",
       " '00087a6bd4dc_06.jpg': [[0, 33, 0, 33], [0, 255, 0, 45]],\n",
       " '00087a6bd4dc_07.jpg': [[167, 255, 0, 35], [127, 255, 0, 17]],\n",
       " '00087a6bd4dc_08.jpg': [[39, 241, 243, 255], [171, 255, 169, 255]],\n",
       " '00089.jpg': [[12, 255, 225, 255], [79, 131, 13, 35]],\n",
       " '00110.jpg': [[0, 255, 0, 68], [135, 255, 0, 63]],\n",
       " '100.png': [[111, 211, 0, 16], [19, 255, 229, 255]],\n",
       " '175.png': [[0, 63, 0, 65], [175, 255, 0, 35]],\n",
       " '179 (1).png': [[0, 69, 8, 87], [0, 255, 225, 255]],\n",
       " '234.jpg': [[175, 255, 0, 78], [183, 255, 0, 55]],\n",
       " '234img.jpg': [[0, 255, 232, 255], [0, 47, 0, 43]],\n",
       " '235.png': [[31, 61, 53, 87], [0, 29, 0, 37]],\n",
       " '242.png': [[47, 151, 25, 95], [211, 255, 143, 176]],\n",
       " '243.png': [[0, 255, 233, 255], [0, 255, 223, 255]],\n",
       " '255.png': [[0, 255, 209, 255], [37, 85, 43, 53]],\n",
       " '257.png': [[155, 189, 220, 237], [143, 229, 237, 255]],\n",
       " '260.png': [[155, 189, 220, 237], [143, 229, 237, 255]],\n",
       " '265.png': [[0, 57, 0, 55], [207, 255, 192, 241]],\n",
       " '269.png': [[215, 255, 79, 161], [0, 95, 193, 255]],\n",
       " '272.png': [[215, 255, 79, 161], [0, 95, 193, 255]],\n",
       " '274.png': [[0, 19, 61, 81], [223, 255, 157, 191]],\n",
       " '281.png': [[187, 239, 57, 141], [41, 73, 53, 85]],\n",
       " '285.png': [[145, 255, 173, 255], [0, 93, 0, 75]],\n",
       " '288.png': [[187, 239, 57, 141], [41, 73, 53, 85]],\n",
       " '296.png': [[215, 255, 79, 161], [0, 95, 193, 255]],\n",
       " '303.png': [[213, 255, 63, 187], [131, 161, 35, 42]],\n",
       " '325.png': [[137, 197, 64, 95], [35, 249, 11, 127]],\n",
       " '359.png': [[129, 151, 35, 41], [0, 119, 11, 53]],\n",
       " '370.png': [[155, 189, 220, 237], [143, 229, 237, 255]],\n",
       " '387.png': [[0, 255, 0, 69], [0, 65, 0, 45]],\n",
       " '62.jpg': [[195, 255, 0, 79], [15, 49, 127, 191]],\n",
       " '89.png': [[0, 255, 225, 255], [0, 49, 7, 101]],\n",
       " '920.jpg': [[31, 80, 161, 193], [0, 39, 80, 171]],\n",
       " 'bad1.jpg': [[0, 27, 61, 123], [83, 97, 247, 249]],\n",
       " 'one1.png': [[105, 151, 31, 49]],\n",
       " 'onewheel.png': [[161, 217, 32, 62], [0, 63, 0, 215]]}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_dict"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
